{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnMDllE4TIx1"
      },
      "source": [
        "# Redes neuronales: Vale, ¿pero de dónde han salido?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhJ0FC2UTIx5"
      },
      "source": [
        "\n",
        "Las redes neuronales están muy de moda ahora, pero para entenderlas de verdad, tenemos que ver su origen. Y su origen, por antiintuitivo que resulte dada su reciente popularidad,\n",
        "está muy atrás, datando de los orígenes del mismísimo Machine Learning.\n",
        "\n",
        "Es 1943. Aparece el primer modelo de neurona artificial, la **neurona McCulloch-Pitts (MCP)**, planteada simplemente como una puerta lógica que recibe estímulos eléctricos.\n",
        "O se dispara o no; o 1 ó 0. Unos cuantos años más tarde, aparece el primer modelo en hacer uso de este concepto de neurona para aprender, en base a una entrada de \"features\",\n",
        "los coeficientes que debía aprender el algoritmo para decidir si una neurona se activaba o no.\n",
        "\n",
        "Este modelo es conocido como el **perceptrón**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-Bo9E2ATIx6"
      },
      "source": [
        "# El Perceptrón"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpwLWR0tTIx7"
      },
      "source": [
        "## Definiciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnyOXG2WTIx7"
      },
      "source": [
        "Tenemos un **vector** $X$ de features, que serán la entrada del algoritmo, y otro vector $W$ de **pesos**, uno por cada feature:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "x_{1}\\\\\n",
        "x_{2}\\\\\n",
        "...\\\\\n",
        "x_{n}\n",
        "\\end{bmatrix}\n",
        "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\\n",
        "W = \\begin{bmatrix}\n",
        "w_{1}\\\\\n",
        "w_{2}\\\\\n",
        "...\\\\\n",
        "w_{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Sobre estos dos vectores aplicaremos una combinación lineal, a la que denominaremos **net input** ó $z$, que no es más que el **producto punto** de $X$ y $W$:\n",
        "\n",
        "$$\n",
        "z = x_{1}w_{1} + x_{2}w_{2} + ... + x_{n}w_{n}\n",
        "$$\n",
        "\n",
        "Dado este **net input** $z$, para un problema de clasificación binaria, donde solo queremos predecir si una variable es de clase 0 ó 1, definimos una **función de decisión**  $\\sigma(z)$ muy sencilla:\n",
        "\n",
        "$\\sigma(z) = \\begin{cases}\n",
        "0 & \\text{si } z \\geq \\theta  \\\\\n",
        "1 & \\text{en otro caso}\n",
        "\\end{cases}$\n",
        "\n",
        "Es decir: si el **net input** entre los vectores $X$ y $W$ es mayor o igual que un determinado valor $\\theta$ que escojamos, diremos que ese ejemplo es de clase 1. De lo contrario, será de\n",
        "clase 0.\n",
        "\n",
        "Para simplificar esta función de decisión, definiremos una **unidad de sesgo** o *bias* $b = -\\theta$, y la añadiremos al net input, de tal manera que se nos quedará así:\n",
        "\n",
        "$z = x_{1}w_{1} + x_{2}w_{2} + ... + x_{n}w_{n} + b$\n",
        "\n",
        "Con esta nueva definición de $z$, podemos dejar la función de decisión $\\sigma$ tal que así:\n",
        "\n",
        "$\\sigma(z) = \\begin{cases}\n",
        "0 & \\text{si } z \\geq 0  \\\\\n",
        "1 & \\text{en otro caso}\n",
        "\\end{cases}$\n",
        "\n",
        "Todo esto está muy bien, pero hasta ahora solo hemos establecido definiciones. Los valores de $X$ nos son dados, pero ¿cómo podemos averiguar los valores de $W$ que consigan discriminar una clase de la otra?\n",
        "\n",
        "Aquí entra el algoritmo del **perceptrón**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6kXLE9ITIx9"
      },
      "source": [
        "## Algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN5sPco-TIx-"
      },
      "source": [
        "En primera instancia, vamos a inicializar el vector $W$ a valores pequeños aleatorios, los que sean.\n",
        "\n",
        "A continuación, a lo largo de $N$ iteraciones, a nuestro perceptrón le vamos a pasar un conjunto $T$ de vectores $X$ tal como los hemos definido anteriormente. Por cada vector $X$ de $T$ (es decir, por cada registro que tengamos para nuestro entrenamiento), vamos a computar a qué clase pertenecería según nuestro vector $W$. Dicha predicción puede ser correcta o errónea.\n",
        "\n",
        "Aquí es donde sucede la magia. Nuestro perceptrón comparará la predicción con la realidad, y actualizará los pesos de $W$ de manera acorde. Concretamente, para cada elemento $w_{j}$ de $W$:\n",
        "\n",
        "$w_{j} := w_{j} + \\Delta{w} \\\\\n",
        "b := b + \\Delta{b}$,\n",
        "\n",
        "donde $\\Delta{w}$ y  $\\Delta{b}$ se definen como:\n",
        "\n",
        "$\\Delta{w} = x_{j}\\eta(\\hat{y_{j}} - y_{j}) \\\\\n",
        "\\Delta{b} = \\eta(\\hat{y_{j}} - y_{j})$,\n",
        "\n",
        "donde $y_{j}$ es el valor **real**, $\\hat{y_{j}}$ es nuestro valor **predicho** para ese registro, y $\\eta$ es el **learning rate**, que determina la velocidad a la que aprenderá nuestro algoritmo.\n",
        "\n",
        "Es importante notar que no hemos definido el número $N$ de iteraciones que haremos sobre nuestro conjunto de vectores. Esta $N$ debe ser un número finito. Esto se debe a que este algoritmo no convergerá nunca a cero errores para conjuntos de datos no separables linealmente. Cada iteración de las $N$ que definamos se denominan **épocas**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo3UfH71TIx-"
      },
      "source": [
        "# Adaline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw39GBD0TIx_"
      },
      "source": [
        "Este modelo es una mejora sobre el perceptrón, con una diferencia (o más bien añadido) fundamental: en vez de computar la actualización de los pesos en base a la predicción de cada registro, lo haremos en base a una **función de activación** y una **función de pérdida**.\n",
        "\n",
        "La función de activación es la función que usaremos para alimentar a la función de pérdida, la cual comparará el valor real del registro que predecimos con la salida de la función de activación, y actualizará los pesos en consecuencia.\n",
        "\n",
        "En el caso de Adaline, la función de activación se define como:\n",
        "\n",
        "$\\sigma(X) = X$\n",
        "\n",
        "Es decir, es simplemente la identidad de la función de entrada. Por otro lado, la función de pérdida en este caso la podemos definir como el **error cuadrático medio** entre el registro real y la salida de la función de activación:\n",
        "\n",
        "$L(w, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y^(i) - \\sigma(X^{(i)}))^2$\n",
        "\n",
        "Por lo tanto, las actualizaciones $\\Delta w$ y $\\Delta b$ se definen como:\n",
        "\n",
        "$\\Delta w = \\eta \\nabla_{w,b} L(w, b)$,\n",
        "\n",
        "Donde $\\nabla_{w,b} L(w, b)$ representa el **descenso de gradiente** de la función de pérdida."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}