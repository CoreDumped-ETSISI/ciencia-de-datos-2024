{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGD4J0eYTKQ2"
      },
      "source": [
        "# Perceptrón multicapa\n",
        "\n",
        "Después del perceptrón de Rosenblatt, durante muchos años se perdió el interés en las redes neuronales, debido a que nadie tenía una solución buena y computacionalmente coherente\n",
        "para entrenarlas cuando estas tenían varias capas. Esto es hasta 1986, cuando se implementó el algoritmo de propagación hacia atrás (**backpropagation**). De este modo, el interés\n",
        "en entender las redes neuronales multicapa se incrementó exponencialmente.\n",
        "\n",
        "Un **perceptrón multicapa** es un modelo de red neuronal donde tenemos una capa de entrada y una de salida, al igual que en el modelo básico, pero además, tenemos **capas ocultas** en medio.\n",
        "El propósito de estas **capas ocultas** es propagar los patrones en los datos de entrada a través de la red.\n",
        "\n",
        "Hay que notar que añadir más capas ocultas no mejora necesariamente el desempeño de un modelo. Se trata de otro **hiperparámetro** a optimizar.\n",
        "\n",
        "El proceso de entrenamiento en un perceptrón multicapa es sencillo:\n",
        "\n",
        "1. Desde la capa de entrada, se propagan los datos hasta la capa de salida.\n",
        "2. En base a la salida, calculamos el error en función de la **función de pérdida** que hayamos definido.\n",
        "3. Aplicamos **propagación hacia atrás** para actualizar los pesos.\n",
        "\n",
        "La forma de propagar hacia delante es sencilla, lo que te esperarías después de haber visto la lección anterior:\n",
        "\n",
        "$z_{1}^{h} = x_{1}^{in} \\cdot w_{1,1}^{h} + x_{2}^{in} \\cdot w_{1,2}^{h} + ... + x_{n}^{in} \\cdot w_{1,n}^{h}$\n",
        "\n",
        "Donde $z_{1}^{h}$ es el **net_input** de la capa $h$, la neurona 1.\n",
        "\n",
        "Ahora debemos calcular la **activación** de dicha neurona, que sería:\n",
        "\n",
        "$a_{1}^{h} = \\sigma (z_{1}^{h})$\n",
        "\n",
        "Donde $\\sigma$ representa la **función de activación**. Como vimos en la sesión pasada, la función de activación puede ser cualquiera que sea diferenciable y que se adapte a nuestras necesidades.\n",
        "Por ejemplo, como ya vimos, si nuestras variables no son linealmente separables, una función sigmoide nos puede valer.\n",
        "\n",
        "Debemos repetir este proceso para $a_{2}, a_{3}...$ y cualquier otra neurona que haya en la capa oculta.\n",
        "\n",
        "Una vez tengamos la activación de cada neurona, deberemos ir a la siguiente capa y realizar el mismo proceso. En este caso, es una salida única con una sola neurona (puede haber más),\n",
        "así que la operación sería:\n",
        "\n",
        "$z_{out}^{h} = a_{1}^{h} \\cdot w_{1}^{h} + a_{2}^{h} \\cdot w_{2}^{h} + ... + x_{n}^{h} \\cdot w_{n}^{h}$\n",
        "\n",
        "$a_{out}^{h} = \\sigma (z_{out}^{h})$\n",
        "\n",
        "Una vez llegamos al final y tenemos nuestra predicción, la comparamos contra nuestra función de pérdida. Para ello, usaremos el **algoritmo de propagación hacia atrás**.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_{1,1}^{out}} = \\frac{\\partial L}{\\partial a_{1}^{out}} \\cdot \\frac{\\partial a_{1}^{out}}{\\partial z_{1}^{out}} \\cdot \\frac{\\partial z_{1}^{out}}{\\partial w_{1,1}^{out}}$\n",
        "\n",
        "Este algoritmo se basa en la **regla de la cadena** para encontrar la derivada parcial de la función de perdida respecto del peso de la capa de salida.\n",
        "\n",
        "Las matemáticas en este caso no son excesivamente importantes, sólo importa que entiendas el concepto básico: entrenar esto **para cada peso de la red** es muy complicado. Necesitaremos algo más potente que una CPU normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ8UATi7TKQ8"
      },
      "source": [
        "# Pytorch\n",
        "\n",
        "Los ejemplos que hemos puesto arriba están simplificados y solo aplican a uno de los muchos caminos (aristas) que hay que recorrer en una red neuronal. Pero imagina por un segundo\n",
        "si nuestra capa oculta tuviese más neuronas. Imagina si quisiésemos meter OTRA capa oculta. Los números escalan rápido. Esto en una CPU normal y corriente explota.\n",
        "\n",
        "Para eso necesitamos **tarjetas gráficas**.\n",
        "\n",
        "La ventaja que tiene una tarjeta gráfica respecto de una CPU normal es el **número de núcleos**. Cada núcleo de la GPU se puede considerar como un pequeño ordenador, a cada uno de los cuales se le puede mandar un proceso distinto haciendo computación en paralelo. Una CPU típica tiene cuatro u ocho núcleos. **Una GPU puede tener cientos o miles.** Imagina si cada una de las operaciones que he explicado arriba, cada uno de los *net inputs* de cada uno de los pesos, así como su respectiva función de activación, pudiesen calcularse en paralelo. Nos ahorraríamos muchísimo tiempo.\n",
        "Redes neuronales que en una CPU tardarían semanas en entrenarse, en una GPU tardarían horas.\n",
        "\n",
        "Sin embargo, hacer un uso eficiente de los recursos de una GPU no es una tarea tan sencilla, por no hablar de que la implementación de esta solución en paralelo es complicada. Existen paquetes\n",
        "que nos permiten usar estos recursos, como por ejemplo CUDA, pero programar con CUDA es complicado.\n",
        "\n",
        "Para eso sirve Pytorch.\n",
        "\n",
        "Pytorch es un framework de deep learning escalable para implementar y ejecutar modelos de redes neuronales complejos de manera muchísimo más sencilla. Pytorch abstrae muchísimos de los conceptos\n",
        "que hemos estado aprendiendo, como la función de activación o el *net_input*, para que nosotros, como científicos de datos, solo debamos preocuparnos por el modelo.\n",
        "\n",
        "A continuación vamos a definir algunos conceptos de Pytorch importantes de entender para utilizarlo correctamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTuN1WtrTKQ9"
      },
      "source": [
        "## [Tensores](https://pytorch.org/docs/stable/tensors.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9k1cTjgTKQ-"
      },
      "source": [
        "Un tensor es una **generalización de un vector n-dimensional**. En la práctica, un tensor lo podemos usar para referirnos a un valor escalar (un 3, un 5...) o a un vector, o a una matriz..., y así sucesivamente. Son la estructura de datos por defecto de Pytorch y se utilizan en todas sus operaciones.\n",
        "\n",
        "Un tensor puede \"vivir\" en la CPU o en la GPU. Podemos especificarlo con los métodos `.cpu()` y `.cuda()`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a crear un tensor de ejemplo:"
      ],
      "metadata": {
        "id": "9XsY42prUhI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tensor_ejemplo = torch.rand(3, 3)\n",
        "print(\"Tensor de ejemplo:\")\n",
        "print(tensor_ejemplo)"
      ],
      "metadata": {
        "id": "q9j4bCdmUNUK",
        "outputId": "0fbbff2a-0c26-45de-f04c-691733b37b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor de ejemplo:\n",
            "tensor([[0.3929, 0.1339, 0.8994],\n",
            "        [0.0242, 0.6998, 0.0354],\n",
            "        [0.1168, 0.6897, 0.5384]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos unas pocas operaciones que podemos realizar con él:"
      ],
      "metadata": {
        "id": "4aOS5KawUm7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos acceder a elementos concretos, como en una matriz\n",
        "elemento = tensor_ejemplo[1, 1]\n",
        "print(f\"Elemento en la fila 1, columna 1: \\n{elemento}\\n\\n\")\n",
        "\n",
        "# podemos hacer operaciones elemento a elemento, como\n",
        "# con las matrices de numpy\n",
        "tensor_a = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor_b = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "suma = tensor_a + tensor_b\n",
        "print(f\"[+] Suma de tensores: \\n{suma}\\n\\n\")\n",
        "\n",
        "# producto de tensores\n",
        "producto = torch.matmul(tensor_a, tensor_b)\n",
        "print(f\"[*] Producto de tensores: \\n{producto}\\n\\n\")\n",
        "\n",
        "# cambio de forma del tensor\n",
        "tensor_reshape = tensor_a.view(1, 4)\n",
        "print(f\"[+] Nuevo tensor: \\n{tensor_reshape}\")"
      ],
      "metadata": {
        "id": "oUB8VbE2Uo7x",
        "outputId": "86027428-5557-49fb-e181-af97608e0366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elemento en la fila 1, columna 1: \n",
            "0.6998239159584045\n",
            "\n",
            "\n",
            "[+] Suma de tensores: \n",
            "tensor([[ 6,  8],\n",
            "        [10, 12]])\n",
            "\n",
            "\n",
            "[*] Producto de tensores: \n",
            "tensor([[19, 22],\n",
            "        [43, 50]])\n",
            "\n",
            "\n",
            "[+] Nuevo tensor: \n",
            "tensor([[1, 2, 3, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La operación más interesante de cara a deep learning son las sumas-producto, o producto escalar (dot product en inglés):"
      ],
      "metadata": {
        "id": "sVIi0ox-V1Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dot_product = torch.dot(tensor_a.view(-1), tensor_b.view(-1))\n",
        "print(f\"[·] Producto escalar: {dot_product}\\n\\n\")"
      ],
      "metadata": {
        "id": "tygUrPVkVu_t",
        "outputId": "9f12628f-cb67-4c42-b8e3-46fc8d2228d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[·] Producto escalar: 70\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Ocn4b9TKQ-"
      },
      "source": [
        "## [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aytWQBchTKQ-"
      },
      "source": [
        "Un DataLoader en PyTorch es una clase que se utiliza para cargar datos en lotes de manera eficiente para su procesamiento en modelos de deep learning. Básicamente, se alimenta de un dataset y nos proporciona un iterador para ir iterando sobre este. El tamaño de cada iteración (o batch) lo podemos especificar en el constructor. [Mira este tutorial sobre cómo usarlo con los datasets](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irHWKSyRTKQ_"
      },
      "source": [
        "## [Clase \"nn.Module\"](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
        "\n",
        "Es la clase base para todos los modelos de redes neuronales. Las redes que creemos nosotros deben heredar de esta clase.\n",
        "\n",
        "Por defecto, esta clase cuenta con un constructor, y un método `forward`. Ese método es donde se realizará la propagación hacia delante de la red."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}